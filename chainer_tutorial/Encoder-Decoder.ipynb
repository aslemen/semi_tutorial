{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoderモデルを書く"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラル機械翻訳用のEncoder-DecoderモデルをChainerで書いてみます。  \n",
    "[公式のexample](https://github.com/chainer/chainer/blob/master/examples/seq2seq/seq2seq.py)を参考にしましたので、そちらも見てみてください。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリのimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラル機械翻訳（NMT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMTにおけるEncoder-Decodeモデルは、主に二つのコンポーネントを持っています。  \n",
    "\n",
    "- Encoder: ソース言語の文を、ベクトルに変換する。\n",
    "- Decoder: Encoderの出力をターゲット言語の文に変換する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下では、それぞれ分けて書いていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、可変長のid系列のリストをembeddingの系列に変換する関数`sequence_embed`を用意しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 可変長単語id系列を可変長単語ベクトル系列へ写像する関数\n",
    "def sequence_embed(embed, xs):\n",
    "    x_len = [len(x) for x in xs]\n",
    "    x_section = np.cumsum(x_len[:-1])\n",
    "    ex = embed(F.concat(xs, axis=0))\n",
    "    exs = F.split_axis(ex, x_section, 0)\n",
    "    return exs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoderモデルを書きます。  \n",
    "入力単語系列をembeddingの系列に変換し、それらをNStepLSTMでencodeします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0fc17403867e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mChain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_source_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedID\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_source_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Chain' is not defined"
     ]
    }
   ],
   "source": [
    "class Encoder(Chain):\n",
    "    def __init__(self, n_layers, n_source_vocab, n_units, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.source_embed = L.EmbedID(in_size=n_source_vocab, out_size=n_units)\n",
    "            self.encoder_lstm = L.NStepLSTM(n_layers=n_layers, in_size=n_units,\n",
    "                                            out_size=n_units, dropout=dropout)\n",
    "            self.n_source_vocab = n_source_vocab\n",
    "            self.n_units = n_units\n",
    "    \n",
    "    def __call__(self, source_xs):\n",
    "        # 単語の系列を単語ベクトルへ\n",
    "        exs = sequence_embed(self.source_embed, source_xs)\n",
    "        \n",
    "        # lstmの初期状態\n",
    "        hx = None\n",
    "        cx = None\n",
    "        \n",
    "        # lstmで各系列をエンコード\n",
    "        hy, cy, ys = self.encoder_lstm(hx, cx, exs)\n",
    "        return hy, cy, ys\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にDecoderを書きます。  \n",
    "DecoderもEmbedIDとNStepLSTMで書いて、あとでEncoder-Decodeモデルとしてまとめたクラスを書くことにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Decoder(Chain):\n",
    "    def __init__(self, n_layers, n_target_vocab, n_units, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.target_embed = L.EmbedID(in_size=n_target_vocab, out_size=n_units)\n",
    "            self.decoder_lstm = L.NStepLSTM(n_layers=n_layers, in_size=n_units,\n",
    "                                            out_size=n_units, dropout=dropout)\n",
    "            self.n_target_vocab = n_target_vocab\n",
    "            self.n_units = n_units\n",
    "        \n",
    "    def __call__(self, hy, cy, target_xs):\n",
    "        # targetの単語系列を単語ベクトルへ\n",
    "        exs = sequence_embed(self.target_embed, target_xs)\n",
    "        \n",
    "        # encoderの出力を受け取り、lstmでデコード\n",
    "        ho, co, os = self.decoder_lstm(hy, cy, exs)\n",
    "        \n",
    "        # 各タイムステップのhidden layerを返す\n",
    "        return ho, co, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoderモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EncoderとDecoderを書いたので、それらをつなげます。\n",
    "\n",
    "NMTではEncoder-Decoderをいきなりひとつのモデルで定義しても問題ないと思いますが、今回はモデルを分けて保存・再利用できるようにするために、別々のクラスで書きました。\n",
    "\n",
    "学習時のloss計算用のメソッドと予測時の翻訳用のメソッドを書くことにします。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UNKとEOSのidを指定しておく\n",
    "UNK = 0\n",
    "EOS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder_Decoder(Chain):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Encoder_Decoder, self).__init__()\n",
    "        with self.init_scope():\n",
    "            # EncoderとDecoderを引数で受け取るようにしておく。\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "            \n",
    "            # Decoderの隠れ状態からtargetボキャブラリーを予測する線形変換用のレイヤー\n",
    "            self.W = L.Linear(self.decoder.n_target_vocab)\n",
    "            # self.xp にはnumpyかcupyが勝手に入る\n",
    "    \n",
    "    def __call__(self, xs, ys):\n",
    "        # xs: sourceの単語idの系列のリスト\n",
    "        # ys: targetの単語idの系列のリスト\n",
    "        \n",
    "        # EOS=1\n",
    "        # targetの単語sequenceにEOSをくっつける。\n",
    "        # 学習時のDecoderの入力においては系列の前に(ys_in)\n",
    "        # 答え合わせの時は系列の後に(ys_out)\n",
    "        eos = self.xp.array([EOS], 'i')\n",
    "        ys_in = [F.concat([eos, y], axis=0) for y in ys]\n",
    "        ys_out = [F.concat([y, eos], axis=0) for y in ys]\n",
    "        \n",
    "        # encode & decode\n",
    "        hy, cy, _ = self.encoder(xs)\n",
    "        _, _, os = self.decoder(hy, cy, ys_in)\n",
    "        \n",
    "        # loss calculation\n",
    "        batch_size = len(xs)\n",
    "        concat_os = F.concat(os, axis=0)\n",
    "        concat_ys_out = F.concat(ys_out, axis=0)\n",
    "        loss = F.sum(F.softmax_cross_entropy(\n",
    "            self.W(concat_os), concat_ys_out, reduce='no')) / batch_size\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def translate(self, xs, max_length = 100):\n",
    "        batch_size = len(xs)\n",
    "        \n",
    "        # 予測なので、backpropやdropoutをしないモードでモデルを動かす\n",
    "        with chainer.no_backprop_mode(), chainer.using_config('train', False):\n",
    "            # sourceの系列のエンコード\n",
    "            h, c, _ = self.encoder(xs)\n",
    "            \n",
    "            # decode時のinput用にbatch_size分のEOS=1を用意\n",
    "            ys = self.xp.full((batch_size,1), 1, 'i')\n",
    "            result = []\n",
    "            \n",
    "            for i in range(max_length):\n",
    "                h, c, ys = self.decoder(h, c, ys)\n",
    "                cys = F.concat(ys, axis=0)\n",
    "                wy = self.W(cys)\n",
    "                ys = self.xp.argmax(wy.data, axis=1).astype('i')\n",
    "                result.append(ys)\n",
    "                ys = F.reshape(ys, (-1, 1)).data\n",
    "            \n",
    "            result = self.xp.stack(result).T\n",
    "            # Remove EOS tags\n",
    "            outs = []\n",
    "            for y in result:\n",
    "                inds = np.argwhere(y == EOS)\n",
    "                if len(inds) > 0:\n",
    "                    y = y[:inds[0, 0]]\n",
    "                outs.append(y)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
