{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoderモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラル機械翻訳用のEncoder-DecoderモデルをChainerで書いてみます。  \n",
    "[公式のexample](https://github.com/chainer/chainer/blob/master/examples/seq2seq/seq2seq.py)を参考にしましたので、そちらも見てみてください。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの記述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリのimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラル機械翻訳（NMT)のEncoder-Decoderモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMTにおけるEncoder-Decodeモデルは、主に二つのコンポーネントを持っています。  \n",
    "\n",
    "- Encoder: ソース言語の文を、ベクトルに変換する。\n",
    "- Decoder: Encoderの出力をターゲット言語の文に変換する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下では、それぞれ分けて書いていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、可変長のid系列のリストをembeddingの系列に変換する関数`sequence_embed`を用意しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 可変長単語id系列を可変長単語ベクトル系列へ写像する関数\n",
    "def sequence_embed(embed, xs):\n",
    "    x_len = [len(x) for x in xs]\n",
    "    x_section = np.cumsum(x_len[:-1])\n",
    "    ex = embed(F.concat(xs, axis=0))\n",
    "    exs = F.split_axis(ex, x_section, 0)\n",
    "    return exs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoderモデルを書きます。  \n",
    "入力単語系列をembeddingの系列に変換し、それらをNStepLSTMでencodeします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(Chain):\n",
    "    def __init__(self, n_layers, n_source_vocab, n_units, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.source_embed = L.EmbedID(in_size=n_source_vocab, out_size=n_units)\n",
    "            self.encoder_lstm = L.NStepLSTM(n_layers=n_layers, in_size=n_units,\n",
    "                                            out_size=n_units, dropout=dropout)\n",
    "            self.n_source_vocab = n_source_vocab\n",
    "            self.n_units = n_units\n",
    "    \n",
    "    def __call__(self, source_xs):\n",
    "        # 単語の系列を単語ベクトルへ\n",
    "        exs = sequence_embed(self.source_embed, source_xs)\n",
    "        \n",
    "        # lstmの初期状態\n",
    "        hx = None\n",
    "        cx = None\n",
    "        \n",
    "        # lstmで各系列をエンコード\n",
    "        hy, cy, ys = self.encoder_lstm(hx, cx, exs)\n",
    "        return hy, cy, ys\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にDecoderを書きます。  \n",
    "DecoderもEmbedIDとNStepLSTMで書いて、あとでEncoder-Decodeモデルとしてまとめたクラスを書くことにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Decoder(Chain):\n",
    "    def __init__(self, n_layers, n_target_vocab, n_units, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.target_embed = L.EmbedID(in_size=n_target_vocab, out_size=n_units)\n",
    "            self.decoder_lstm = L.NStepLSTM(n_layers=n_layers, in_size=n_units,\n",
    "                                            out_size=n_units, dropout=dropout)\n",
    "            self.n_target_vocab = n_target_vocab\n",
    "            self.n_units = n_units\n",
    "        \n",
    "    def __call__(self, hy, cy, target_xs):\n",
    "        # targetの単語系列を単語ベクトルへ\n",
    "        exs = sequence_embed(self.target_embed, target_xs)\n",
    "        \n",
    "        # encoderの出力を受け取り、lstmでデコード\n",
    "        ho, co, os = self.decoder_lstm(hy, cy, exs)\n",
    "        \n",
    "        # 各タイムステップのhidden layerを返す\n",
    "        return ho, co, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoderモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EncoderとDecoderを書いたので、それらをつなげます。\n",
    "\n",
    "NMTではEncoder-Decoderをいきなりひとつのモデルで定義しても問題ないと思いますが、今回はモデルを分けて保存・再利用できるようにするために、別々のクラスで書きました。\n",
    "\n",
    "学習時のloss計算用のメソッドと予測時の翻訳用のメソッドを書くことにします。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UNKとEOSのidを指定しておく\n",
    "UNK = 0\n",
    "EOS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder_Decoder(Chain):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Encoder_Decoder, self).__init__()\n",
    "        with self.init_scope():\n",
    "            # EncoderとDecoderを引数で受け取るようにしておく。\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "            \n",
    "            # Decoderの隠れ状態からtargetボキャブラリーを予測する線形変換用のレイヤー\n",
    "            self.W = L.Linear(self.decoder.n_target_vocab)\n",
    "            # self.xp にはnumpyかcupyが勝手に入る\n",
    "    \n",
    "    def __call__(self, xs, ys):\n",
    "        # xs: sourceの単語idの系列のリスト\n",
    "        # ys: targetの単語idの系列のリスト\n",
    "        \n",
    "        # ソース言語を反転させて学習させるならここでそういった処理を入れましょう。\n",
    "        \n",
    "        # EOS=1\n",
    "        # targetの単語sequenceにEOSをくっつける。\n",
    "        # 学習時のDecoderの入力においては系列の前に(ys_in)\n",
    "        # 答え合わせの時は系列の後に(ys_out)\n",
    "        eos = self.xp.array([EOS], 'i')\n",
    "        ys_in = [F.concat([eos, y], axis=0) for y in ys]\n",
    "        ys_out = [F.concat([y, eos], axis=0) for y in ys]\n",
    "        \n",
    "        # encode & decode\n",
    "        hy, cy, _ = self.encoder(xs)\n",
    "        _, _, os = self.decoder(hy, cy, ys_in)\n",
    "        \n",
    "        # loss calculation\n",
    "        batch_size = len(xs)\n",
    "        concat_os = F.concat(os, axis=0)\n",
    "        concat_ys_out = F.concat(ys_out, axis=0)\n",
    "        loss = F.sum(F.softmax_cross_entropy(\n",
    "            self.W(concat_os), concat_ys_out, reduce='no')) / batch_size\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def translate(self, xs, max_length = 100):\n",
    "        batch_size = len(xs)\n",
    "        \n",
    "        # 予測なので、backpropやdropoutをしないモードでモデルを動かす\n",
    "        with chainer.no_backprop_mode(), chainer.using_config('train', False):\n",
    "            # sourceの系列のエンコード\n",
    "            h, c, _ = self.encoder(xs)\n",
    "            \n",
    "            # decode時のinput用にbatch_size分のEOS=1を用意\n",
    "            ys = self.xp.full((batch_size,1), 1, 'i')\n",
    "            result = []\n",
    "            \n",
    "            for i in range(max_length):\n",
    "                h, c, ys = self.decoder(h, c, ys)\n",
    "                cys = F.concat(ys, axis=0)\n",
    "                wy = self.W(cys)\n",
    "                ys = self.xp.argmax(wy.data, axis=1).astype('i')\n",
    "                result.append(ys)\n",
    "                ys = F.reshape(ys, (-1, 1)).data\n",
    "            \n",
    "            result = self.xp.stack(result).T\n",
    "            # Remove EOS tags\n",
    "            outs = []\n",
    "            for y in result:\n",
    "                inds = np.argwhere(y == EOS)\n",
    "                if len(inds) > 0:\n",
    "                    y = y[:inds[0, 0]]\n",
    "                outs.append(y)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実際に動かす"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に上で記述したEncoder-Decoderモデルを動かしてみましょう。  \n",
    "\n",
    "まずはデータを用意します。  \n",
    "今回はごく小規模なデータにちゃんとフィッティングできるかを確かめるだけにします。  \n",
    "[日英中基本文データ](http://nlp.ist.i.kyoto-u.ac.jp/index.php?日英中基本文データ)から１００個の日英対訳文をとってきて、日本語はMecabで分かち書き、英語は小文字化を行いました。  \n",
    "それぞれ`ja.txt`、`en.txt`として、dataディレクトリに入れてあります。  \n",
    "このデータを学習してみます。  \n",
    "\n",
    "英日翻訳を試してみます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ読み込み、前処理\n",
    "まずはデータを見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/ja.txt','r') as f:\n",
    "    ja = f.read().strip().split('\\n')\n",
    "\n",
    "with open('data/en.txt','r') as f:\n",
    "    en = f.read().strip().split('\\n')\n",
    "    \n",
    "ja = [s.strip() for s in ja]\n",
    "en = [s.strip() for s in en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X で は ない か と つくづく 疑問 に 思う i often wonder if it might be x.\n",
      "X が いい な と いつも 思い ます i always think x would be nice.\n",
      "それ が ある よう に いつも 思い ます it always seems like it is there.\n",
      "それ が 多 すぎ ない か と 正直 思う i honestly feel like there is too much.\n",
      "山田 は みんな に 好か れる タイプ の 人 だ と 思う i think that yamada is the type everybody likes.\n",
      "〜 と 誰 か が 思っ た someone thought that 〜\n",
      "X は しんどい こと だ と 思い ます x seems like it's really tough.\n",
      "X は 時間 の 問題 と 思い ます i think x is just a matter of time.\n",
      "X は 今後 の 課題 と 思い ます i think that x will become an issue in the future.\n",
      "それ は 桃山 時代 前後 の 作品 だ と 思い ます i think this was made around the momoyama period.\n"
     ]
    }
   ],
   "source": [
    "for j, e in zip(ja[:10], en[:10]):\n",
    "    print(j, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このような文のペアが１００個あります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は単語を元に翻訳を行います。  \n",
    "それぞれの単語をidに置き換えて、idの系列をモデルに入力していくことになります。  \n",
    "よって、単語とidの辞書を作る必要があります。  \n",
    "訓練データに出てくる単語にUNK(UNKNOWN)とEOS(End of Sentence)のトークンを付け加えた辞書を作りましょう。  \n",
    "今回はUNKは0、EOSには1のidを振ります。 \n",
    "\n",
    "※ 今回はvalidation/test dataでの性能評価は行わないので、実はUNKをつける意味はありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ja_w2id = {'UNK':0, 'EOS':1}\n",
    "en_w2id = {'UNK':0, 'EOS':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ja_vocab = set()\n",
    "for s in ja:\n",
    "    ja_vocab.update(s.split(' '))\n",
    "    \n",
    "en_vocab = set()\n",
    "for s in en:\n",
    "    en_vocab.update(s.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298 345\n"
     ]
    }
   ],
   "source": [
    "print(len(ja_vocab), len(en_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, w in enumerate(ja_vocab):\n",
    "    ja_w2id[w] = i+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, w in enumerate(en_vocab):\n",
    "    en_w2id[w] = i+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "英語と日本語、それぞれの単語をidに置き換える辞書ができました。  \n",
    "保存しておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/ja_w2id.dump', 'wb') as f:\n",
    "    pickle.dump(ja_w2id, f)\n",
    "\n",
    "with open('data/en_w2id.dump', 'wb') as f:\n",
    "    pickle.dump(en_w2id, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それは文のリストを単語id系列のリストに変換します。  \n",
    "numpy.arrayで単語id系列のリストにしていきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ja_data = []\n",
    "for s in ja:\n",
    "    s = s.split(' ')\n",
    "    s = [ja_w2id[w] for w in s]\n",
    "    ja_data.append(np.array(s, 'i'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data = []\n",
    "for s in en:\n",
    "    s = s.split(' ')\n",
    "    s = [en_w2id[w] for w in s]\n",
    "    en_data.append(np.array(s, 'i'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([129, 160, 261,   7, 262,  16, 139, 144, 108, 163], dtype=int32), array([312, 253,   5, 292, 196, 251, 213, 270], dtype=int32))\n",
      "(array([129,  39, 226,  15,  16,  44, 193,  30], dtype=int32), array([312, 346,  19, 113, 135, 213,  51], dtype=int32))\n",
      "(array([238,  39,  13,  89, 108,  44, 193,  30], dtype=int32), array([196, 346, 290,  58, 196, 216,  23], dtype=int32))\n",
      "(array([238,  39, 280, 272,   7, 262,  16,  63, 163], dtype=int32), array([312,  33,  47,  58,  35, 216, 223,  90], dtype=int32))\n",
      "(array([ 23, 261,  93, 108,  76, 133, 206, 281,  79, 260,  16, 163], dtype=int32), array([312,  19,  24, 258, 216,  29, 162, 277, 311], dtype=int32))\n",
      "(array([298,  16, 297, 262,  39,  17,  27], dtype=int32), array([108, 206,  24,  87], dtype=int32))\n",
      "(array([129, 261,  22, 290, 260,  16, 193,  30], dtype=int32), array([113, 290,  58,  71, 180, 245], dtype=int32))\n",
      "(array([129, 261, 215, 281,  37,  16, 193,  30], dtype=int32), array([312,  19, 113, 216, 282, 112, 302, 262, 332], dtype=int32))\n",
      "(array([129, 261,  43, 281, 293,  16, 193,  30], dtype=int32), array([312,  19,  24, 113, 115,  97, 309, 107, 313,  29, 225], dtype=int32))\n",
      "(array([238, 261,  60, 264, 174, 281, 198, 260,  16, 193,  30], dtype=int32), array([312,  19, 172, 241,  10,  89,  29, 133, 267], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "for i in zip(ja_data[:10], en_data[:10]):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これがモデルへの入力になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル用意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルを準備します。  \n",
    "今回は以下のようなモデルを学習しましょう。\n",
    "\n",
    "- レイヤー数は1\n",
    "- embedding, LSTMの中間層の次元は50。\n",
    "- dropout rateは0.2\n",
    "- 最適化はAdamで行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 英語の文のEncoder\n",
    "encoder = Encoder(n_layers=1, n_source_vocab=len(en_w2id), n_units=50, dropout=0.2)\n",
    "\n",
    "# 日本語の文のDecoder\n",
    "decoder = Decoder(n_layers=1, n_target_vocab=len(ja_w2id), n_units=50, dropout=0.2)\n",
    "\n",
    "# Encoder-Decoderモデル。\n",
    "model = Encoder_Decoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "試しにちょっとだけデータを食わせてみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable(56.64057540893555)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(en_data[:2], ja_data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lossの値が返ってくるのがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最適化のためのoptimizerを定義しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(0.01)\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、学習のための準備が整いました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20epochほど回してみて、5epochごとにどれくらいうまくフィッティングできているかを確かめましょう。  \n",
    "ミニバッチサイズは10とします。  \n",
    "今回は最初の3文を翻訳してみて、確かめてみます。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_translate_en = en_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "翻訳用の関数を用意しましょう。  \n",
    "idから単語に戻すための辞書も用意しておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ja_id2w = {i:w for w, i in ja_w2id.items()}\n",
    "en_id2w = {i:w for w, i in en_w2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_print(en_array, ja_array, en_id2w, ja_id2w):\n",
    "    print('=====中間報告=====')\n",
    "    for en, ja in zip(en_array, ja_array):\n",
    "        en_s = [en_id2w[i] for i in en]\n",
    "        print(' '.join(en_s))\n",
    "        \n",
    "        ja_s = [ja_id2w[i] for i in ja]\n",
    "        print('>>> '+' '.join(ja_s))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====中間報告=====\n",
      "i often wonder if it might be x.\n",
      ">>> X で は ない か と つくづく 疑問 に 思う\n",
      "i always think x would be nice.\n",
      ">>> X が いい な と いつも 思い ます\n",
      "it always seems like it is there.\n",
      ">>> それ が ある よう に いつも 思い ます\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate_print(en_data[:3], ja_data[:3], en_id2w, ja_id2w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "動いてます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは学習を開始してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# epoch: 0, loss: 522.3270835876465\n",
      "# epoch: 1, loss: 367.9393253326416\n",
      "# epoch: 2, loss: 301.5761775970459\n",
      "# epoch: 3, loss: 254.57226181030273\n",
      "# epoch: 4, loss: 211.6100616455078\n",
      "=====中間報告=====\n",
      "i often wonder if it might be x.\n",
      ">>> X は は の と 思い ます\n",
      "i always think x would be nice.\n",
      ">>> X が X を し た\n",
      "it always seems like it is there.\n",
      ">>> それ が の を する\n",
      "\n",
      "\n",
      "# epoch: 5, loss: 172.45455741882324\n",
      "# epoch: 6, loss: 136.8543004989624\n",
      "# epoch: 7, loss: 105.37115383148193\n",
      "# epoch: 8, loss: 79.51851654052734\n",
      "# epoch: 9, loss: 59.56546878814697\n",
      "=====中間報告=====\n",
      "i often wonder if it might be x.\n",
      ">>> X で は ない か と つくづく 疑問 に 思う\n",
      "i always think x would be nice.\n",
      ">>> X が いい な と いつも 思い ます\n",
      "it always seems like it is there.\n",
      ">>> それ が ある よう に いつも 思い ます\n",
      "\n",
      "\n",
      "# epoch: 10, loss: 44.27755522727966\n",
      "# epoch: 11, loss: 33.51788568496704\n",
      "# epoch: 12, loss: 25.33184826374054\n",
      "# epoch: 13, loss: 19.647087335586548\n",
      "# epoch: 14, loss: 15.441965579986572\n",
      "=====中間報告=====\n",
      "i often wonder if it might be x.\n",
      ">>> X で は ない か と つくづく 疑問 に 思う\n",
      "i always think x would be nice.\n",
      ">>> X が いい な と いつも 思い ます\n",
      "it always seems like it is there.\n",
      ">>> それ が ある よう に いつも 思い ます\n",
      "\n",
      "\n",
      "# epoch: 15, loss: 12.437528491020203\n",
      "# epoch: 16, loss: 10.313651502132416\n",
      "# epoch: 17, loss: 8.308997750282288\n",
      "# epoch: 18, loss: 7.05184531211853\n",
      "# epoch: 19, loss: 6.045078456401825\n",
      "=====中間報告=====\n",
      "i often wonder if it might be x.\n",
      ">>> X で は ない か と つくづく 疑問 に 思う\n",
      "i always think x would be nice.\n",
      ">>> X が いい な と いつも 思い ます\n",
      "it always seems like it is there.\n",
      ">>> それ が ある よう に いつも 思い ます\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 訓練データ総数\n",
    "n_train = len(ja_data)\n",
    "\n",
    "# ミニバッチサイズ\n",
    "batchsize = 10\n",
    "\n",
    "# エポック数\n",
    "n_epoch =20\n",
    "\n",
    "# ミニバッチ化のために、array化しておく。dtypeはobject\n",
    "ja_data = np.array(ja_data)\n",
    "en_data = np.array(en_data)\n",
    "\n",
    "for epoch in range(n_epoch): # epochのループ\n",
    "    sum_loss = 0\n",
    "    # ミニバッチの用意。\n",
    "    perm = np.random.permutation(n_train)\n",
    "    for i in range(0, n_train, batchsize):\n",
    "        ja_batch = ja_data[perm[i:i+batchsize]]\n",
    "        en_batch = en_data[perm[i:i+batchsize]]\n",
    "        \n",
    "        # lossの計算\n",
    "        loss = model(en_batch, ja_batch)\n",
    "        sum_loss += loss.data\n",
    "        # backpropatgation\n",
    "        optimizer.target.cleargrads() # 勾配のリセット\n",
    "        loss.backward() # 逆伝搬する誤差を計算\n",
    "        optimizer.update() # パラメータを更新\n",
    "    \n",
    "    log = '# epoch: {}, loss: {}'.format(epoch+1, sum_loss)\n",
    "    print(log)\n",
    "    \n",
    "    # 5epochごとに翻訳を出力。\n",
    "    if (epoch+1)%5 == 0:\n",
    "        result = model.translate(for_translate_en)\n",
    "        translate_print(for_translate_en, result, en_id2w, ja_id2w)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初の5epochではダメな感じですが、訓練データに含まれている文なので、さすがに次の5epochではフィッティングできていますね。  \n",
    "lossもちゃんと減っていっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は学習にあまり時間をかけないようにするために、小さな訓練データを用いてモデルを訓練し、訓練データの文を使ってフィッティングを検証していますが、  \n",
    "\n",
    "実際の機械学習では、ちゃんとtrain/val/testにデータを分割しなければなりません。    \n",
    "単語id辞書作成や学習は訓練データのみで行い、valデータでモデル選択、テストデータでモデルの評価を行うようにしてください。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "お疲れ様でした。  \n",
    "ChainerでEncoder-Decoderモデルを定義し、小規模データを使った機械翻訳の学習を行いました。  \n",
    "次は今回やったことの発展として、Attentionを導入します。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
