{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder-Decoderモデルの発展系として、Attentionを追加してみます。  \n",
    "ついでにEncocderのLSTMもBidirectional LSTMに変えてみましょう。\n",
    "\n",
    "残念ながら、2017年10月8日時点で、Attentionを一行で追加してくれるような機能はChainerにはありません。  \n",
    "Attentionの構造とChainerの関数などをきちんと理解し、自分で実装していくことになります。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ライブラリのインポート、idの系列をembeddingの系列に変える関数は先と一緒です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可変長単語id系列を可変長単語ベクトル系列へ写像する関数\n",
    "def sequence_embed(embed, xs):\n",
    "    x_len = [len(x) for x in xs]\n",
    "    x_section = np.cumsum(x_len[:-1])\n",
    "    ex = embed(F.concat(xs, axis=0))\n",
    "    exs = F.split_axis(ex, x_section, 0)\n",
    "    return exs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "せっかくAttentionを使うので、EncoderのLSTMをBidirectionalにしてみます。  \n",
    "Bidirectional LSTMは、すでにChainerに用意されているので、そこを変えるだけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Chain):\n",
    "    def __init__(self, n_layers, n_source_vocab, n_units, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.source_embed = L.EmbedID(in_size=n_source_vocab, out_size=n_units)\n",
    "            \n",
    "            # NStepLSTMをNStepBiLSTMに\n",
    "            self.encoder_lstm = L.NStepBiLSTM(n_layers=n_layers, in_size=n_units,\n",
    "                                              out_size=n_units, dropout=dropout)\n",
    "\n",
    "            self.n_source_vocab = n_source_vocab\n",
    "            self.n_units = n_units\n",
    "    \n",
    "    def __call__(self, source_xs):\n",
    "        # 単語の系列を単語ベクトルへ\n",
    "        exs = sequence_embed(self.source_embed, source_xs)\n",
    "        \n",
    "        # lstmの初期状態\n",
    "        hx = None\n",
    "        cx = None\n",
    "        \n",
    "        # lstmで各系列をエンコード\n",
    "        hy, cy, ys = self.encoder_lstm(hx, cx, exs)\n",
    "        return hy, cy, ys\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意としては、出力のhy, cyのshapeが変わります。  \n",
    "n_layerのaxisが、forward LSTMとbackward LSTMの分を合わせ、n_layer * 2になります。  \n",
    "また、ysの各隠れ層の次元数も2倍になります。  \n",
    "よってDecoderのLSTMの隠れ層の次元数はEmbeddingやEncoderのn_unitsの二倍になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attentionを導入します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はAttentionが機械翻訳に初めて適用された、 \n",
    "\n",
    "・[Bahdanau et al. (2015) Neural Machine Translation by Jointly Learning Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)  \n",
    "\n",
    "のモデルを書いていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://raw.githubusercontent.com/kwashio/semi_tutorial/images/attention.png width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像は[スタンフォード大学の授業のスライド](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture10.pdf)から拝借しました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bahdanauのモデルでは、AttentionはLSTMの一番深い層（画像では一番上の層）で展開され、Decoderの隠れ状態を計算する際に使用されます。  \n",
    "具体的には、$h_t$を計算する際に、\n",
    "\n",
    "1. 一つ前の隠れ状態$h_{t-1}$からContext vector $c_t$を計算\n",
    "1. $h_{t-1}$、$c_t$、input vector（画像だと$h_t$の下のベクトル）から$h_t$を計算\n",
    "\n",
    "という風になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のクラスAttentionでは、$h_{t-1}$から$a_t$を計算し、$c_t$を出力するまでの処理を記述しています。 \n",
    "\n",
    "式にすると、Encoderのある隠れ状態の$score_i$は、\n",
    "\n",
    "\\begin{equation}\n",
    "score_i = softmax(W_2 z_i) \\\\\n",
    "z_i = tanh(W_1 (e_i \\oplus h_{t-1}))\n",
    "\\end{equation}\n",
    "\n",
    "ただし、$e_i$はEncoderの時点$i$における隠れ状態です。$\\oplus$はベクトルの結合を表しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Chain):\n",
    "    def __init__(self, n_units):\n",
    "        super(Attention, self).__init__()\n",
    "        with self.init_scope():\n",
    "            #eWとdWで上の式のW1を表している。\n",
    "            # Encoder(BiLSTM)の隠れ状態の線形変換\n",
    "            self.eW = L.Linear(n_units*2) # Decoderの隠れ層の次元数はEncoderのn_unitsの二倍\n",
    "            \n",
    "            # 一つ前のdecoder中間層の線形変換\n",
    "            self.dW = L.Linear(n_units*2)\n",
    "            \n",
    "            \n",
    "            # スコア計算用の線形変換、上の式のW2\n",
    "            self.aW = L.Linear(1)\n",
    "    \n",
    "    def __call__(self, ehs, dh):\n",
    "        # 各z_iの計算\n",
    "        encoder_hidden = self.eW(ehs)\n",
    "        \n",
    "        # h_{t-1}の線形変換後のベクトルをbroadcastし、コピーしてencoder_hiddenに足し合わせる。\n",
    "        decoder_hidden = F.broadcast_to(self.dW(dh), encoder_hidden.shape)\n",
    "        attention_hidden = F.tanh(encoder_hidden + decoder_hidden)\n",
    "        \n",
    "        # scoreの計算。\n",
    "        scores = F.softmax(self.aW(attention_hidden), axis=0)\n",
    "        \n",
    "        # context vectorの計算\n",
    "        context = F.matmul(F.transpose(scores), ehs)\n",
    "        # (1 , n_units*2)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder(with Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attentionを考慮したDecoderを書いていきます。  \n",
    "少し複雑ですが、頑張ってついてきてください。  \n",
    "\n",
    "Attentionなしの単純なEncoder-Decoderモデルを書いたときは、EncoderもDecoderもNStepLSTMで書くことができました。  \n",
    "しかし、Attentionを考慮する場合は、Decoderの一番深いレイヤーの各隠れ状態を計算する際に、context vector $c_t$を計算に入れる必要があるため、単純にNStepLSTMを用いることはできません。\n",
    "\n",
    "つまり、一番深いレイヤーの計算部分は自分で書かなければなりません。  \n",
    "もう一度、さきほどの図を見てみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://github.com/kwashio/semi_tutorial/blob/images/attention2.png?raw=true width=500px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$c_t$を除く青い隠れ状態の部分はEncoderで計算済みです。  \n",
    "赤色の隠れ状態はDecoderで計算するのですが、多層のLSTMを考えた時、赤枠の部分はNStepLSTMで計算できます。  \n",
    "つまり、自分で書かなければいけないのは、Decoderのトップの層の部分ということになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoderもトップの層もLSTMなのですが、通常のLSTMとは異なり、$h_{t-1}$とinput以外にcontext vector $c_t$を考慮したLSTMです。  \n",
    "このようなLSTMを記述する際は、レイヤーのLSTMやNStepLSTMではなく、[chainer.functions.lstm](https://docs.chainer.org/en/stable/reference/generated/chainer.functions.lstm.html#chainer.functions.lstm)を使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functionのLSTMは、大雑把に入力と出力を書くと、\n",
    "```\n",
    "c, h = lstm(previous_c, W1*previous_h + W2*input)\n",
    "```\n",
    "という風になっています。つまり、一つ前のcellと隠れ状態、input vectorを渡すと、新しいcellと隠れ状態を返してくれる関数です。  \n",
    "注意点としては、previous_h、 input vectorはlstm関数に入力される前に、Linearレイヤー（W1, W2）により中間層の4倍の次元のベクトルに変換されなければないことです。  \n",
    "なぜ4倍なのかというと、これはLSTMの各構成要素である、input gate、forget gate、output gate、new memory cellの計算に対応しています。\n",
    "\n",
    "このlstm関数はレイヤーのLSTMと異なり、内部にパラメータを持っておらず、計算処理のみを担っています。  \n",
    "LSTMとしてのパラメータは上の式における、W1とW2に相当します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ざっくりとlstm関数を理解したところで、ここにcontext vector $c_t$を組み込みます。  \n",
    "これは、以下のように行います。\n",
    "\n",
    "```\n",
    "c, h = lstm(previous_c, W1*previous_h + W2*input + W3*context)\n",
    "```\n",
    "\n",
    "これにより、context vectorを考慮しつつ、新たなcellと隠れ状態を計算することができます。  \n",
    "では、Decoderクラスを書いていきましょう。\n",
    "後々の処理過程を前に書いたAttentionなしのEncoder-Decoderモデルに合わせるため、出力はNStepLSTMと同じになるようにします。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Chain):\n",
    "    \n",
    "    def __init__(self, n_layers, n_target_vocab, n_units, attention, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.n_target_vocab = n_target_vocab\n",
    "            self.target_embed = L.EmbedID(n_target_vocab, n_units, dropout)\n",
    "            self.n_layers = n_layers\n",
    "            \n",
    "            # layer数が１かそれ以上かで、NStepLSTMを使うかどうか分岐\n",
    "            if self.n_layers > 1:\n",
    "                self.pre_lstm = L.NStepLSTM(self.n_layers -1, n_units, n_units*2, dropout)\n",
    "            \n",
    "            # attention\n",
    "            self.Att = attention\n",
    "            \n",
    "            self.dropout = dropout\n",
    "            \n",
    "            # topのLSTMの各線形変換（W1, W2, W3）\n",
    "            # 中間層がn_units*2なので、それを４倍にする。\n",
    "            self.lstm_input = L.Linear(n_units * 8)\n",
    "            self.lstm_previous = L.Linear(n_units * 8)\n",
    "            self.lstm_context = L.Linear(n_units * 8)\n",
    "            \n",
    "        \n",
    "    def __call__(self, hy, cy, ys, target_xs):\n",
    "        # hy, cy, ysはEncoder(BiLSTM)の出力\n",
    "        # target_xsは、単語idの系列のリスト\n",
    "        \n",
    "        # attention以外の部分の計算\n",
    "        batch_size = len(ys)\n",
    "        \n",
    "        # ターゲット言語の単語idの系列のリストを、embeddingの系列のリストへ\n",
    "        exs = sequence_embed(self.target_embed, target_xs)\n",
    "        \n",
    "        # EncoderのBiLSTMのhy, cyのshapeをDecoderの構造に合わせる。\n",
    "        # (n_layers*2, batchsize, n_units) -> (n_layers, batchsize, n_units*2)へ\n",
    "        hy = F.reshape(hy, (self.n_layers, batch_size, -1))\n",
    "        cy = F.reshape(cy, (self.n_layers, batch_size, -1))\n",
    "        \n",
    "        # n_layersが２以上のときは、NStepLSTMにより、一番深い層以外の隠れ状態を計算しておく。\n",
    "        if self.n_layers > 1:\n",
    "            unatt_n_layer = self.n_layers - 1\n",
    "            pre_hy = hy[:unatt_n_layer]\n",
    "            pre_cy = cy[:unatt_n_layer]\n",
    "            after_h, after_c, pre_os = self.pre_lstm(pre_hy, pre_cy, exs)\n",
    "        \n",
    "        else:\n",
    "            pre_os = exs\n",
    "        # pre_osが、一番深い層のLSTMへのinputの系列になる。\n",
    "        \n",
    "        # 最終層の計算\n",
    "        high_hy = hy[self.n_layers - 1]\n",
    "        high_cy = cy[self.n_layers - 1]\n",
    "        \n",
    "        # NStepLSTMと出力を合わせるために、リストを３つ用意\n",
    "        last_h = [] # 最後の隠れ状態のリスト\n",
    "        last_c = [] # 最後のcellのリスト\n",
    "        os = [] # 隠れ状態の系列のリスト\n",
    "        \n",
    "        # 各input系列ごとに処理\n",
    "        for i, pre_eos in enumerate(pre_os):\n",
    "            h = F.reshape(high_hy[i], (1,-1))\n",
    "            c = F.reshape(high_hy[i], (1,-1))\n",
    "            now_ys = ys[i] # 対応するEncoderの隠れ状態の系列 (lenght, n_units*2)\n",
    "            temp_os = []\n",
    "            \n",
    "            # verticalにdropoutがかかるので、dropoutをかける場所はここ\n",
    "            pre_eos = F.dropout(pre_eos, self.dropout)\n",
    "            \n",
    "            \n",
    "            for x in pre_eos:\n",
    "                # input vector\n",
    "                x = F.reshape(x, (1,-1))\n",
    "                \n",
    "                # 一つ前の隠れ状態からcontext vectorを計算。\n",
    "                context = self.Att(now_ys, h)\n",
    "                \n",
    "                # 次のセルと隠れ状態を計算する。\n",
    "                c, h = F.lstm(c, self.lstm_input(x) + self.lstm_previous(h) + self.lstm_context(context))\n",
    "                \n",
    "                # 隠れ状態を保存\n",
    "                temp_os.append(h)\n",
    "            \n",
    "            # 最後の隠れ状態、最後のセル、隠れ状態の系列を保存\n",
    "            last_h.append(h)\n",
    "            last_c.append(c)\n",
    "            os.append(F.concat(temp_os, axis=0))\n",
    "        \n",
    "        # 出力をNStepLSTMに合わせるために、shapeを変換\n",
    "        last_h = F.reshape(F.concat(last_h, axis=0), (1, batch_size, -1))\n",
    "        last_c = F.reshape(F.concat(last_c, axis=0), (1, batch_size, -1))\n",
    "        \n",
    "        # n_layerが２以上のときは、NStepLSTMの出力とconcatする。\n",
    "        if self.n_layers > 1:\n",
    "            ho = F.concat([after_h, last_h], axis=0)\n",
    "            co = F.concat([after_c, last_c], axis=0)\n",
    "        else:\n",
    "            ho = last_h\n",
    "            co = last_c\n",
    "            \n",
    "        \n",
    "        return ho, co, os\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "お疲れ様でした。  \n",
    "頑張って出力の形を揃えたので、あとの処理はAttentionなしのEncoder-Decoderモデルとほぼ同じです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = 0\n",
    "EOS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Decoder_withAttention(Chain):\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Encoder_Decoder_withAttention, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "        \n",
    "            self.W = L.Linear(self.decoder.n_target_vocab)\n",
    "            \n",
    "    def __call__(self, xs, ys):\n",
    "        eos = self.xp.array([EOS], 'i')\n",
    "        ys_in = [F.concat([eos, y], axis=0) for y in ys]\n",
    "        ys_out = [F.concat([y, eos], axis=0) for y in ys]\n",
    "        \n",
    "        hy, cy, ys = self.encoder(xs)\n",
    "        _, _, os = self.decoder(hy, cy, ys, ys_in)\n",
    "        \n",
    "        # loss calculation\n",
    "        batch_size = len(xs)\n",
    "        concat_os = F.concat(os, axis=0)\n",
    "        concat_ys_out = F.concat(ys_out, axis=0)\n",
    "        loss = F.sum(F.softmax_cross_entropy(\n",
    "            self.W(concat_os), concat_ys_out, reduce='no')) / batch_size\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def translate(self, xs, max_length = 100):\n",
    "        batch_size = len(xs)\n",
    "        \n",
    "        with chainer.no_backprop_mode(), chainer.using_config('train', False):\n",
    "            # Encode\n",
    "            hy, cy, ys = self.encoder(xs)\n",
    "            \n",
    "            # decode時のinput用にbatch_size分のEOS=1を用意\n",
    "            target_xs = self.xp.full((batch_size,1), 1, 'i')\n",
    "            result = []\n",
    "            \n",
    "            ho = hy\n",
    "            co = cy\n",
    "            for i in range(max_length):\n",
    "                ho, co, os = self.decoder(ho, co, ys, target_xs) \n",
    "                concat_os = F.concat(os, axis=0)\n",
    "                wy = self.W(concat_os)\n",
    "                target_xs = self.xp.argmax(wy.data, axis=1).astype('i')\n",
    "                result.append(target_xs)\n",
    "                target_xs = F.reshape(target_xs, (-1, 1)).data\n",
    "            \n",
    "            result = self.xp.stack(result).T\n",
    "            # Remove EOS tags\n",
    "            outs = []\n",
    "            for y in result:\n",
    "                inds = np.argwhere(y == EOS)\n",
    "                if len(inds) > 0:\n",
    "                    y = y[:inds[0, 0]]\n",
    "                outs.append(y)\n",
    "        return outs\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder  \n",
    "target言語の系列のリストと、一つ前の隠れ状態、セルを受け取る。  \n",
    "各タイムステップの隠れ状態を返す。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力: xs = [np.array(1,3,4...), np.array(...),...], h, c  \n",
    "出力: os = [(len, n_units), (len, n_units), ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h = (1, n_units by 2)\n",
    "c = (1, n_units by 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [np.random.randint(0, 50, size=np.random.randint(1, 15, 1)).astype('i')\n",
    "      for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 4, 38,  5, 42], dtype=int32),\n",
       " array([35, 12, 26], dtype=int32),\n",
       " array([20, 17,  4], dtype=int32),\n",
       " array([ 4, 29, 19,  0, 46, 16, 45, 32, 27, 41], dtype=int32),\n",
       " array([20,  0], dtype=int32),\n",
       " array([44, 13], dtype=int32),\n",
       " array([36, 15], dtype=int32),\n",
       " array([23, 16], dtype=int32),\n",
       " array([43, 21, 15, 41, 23, 14, 39], dtype=int32),\n",
       " array([27, 34, 26], dtype=int32),\n",
       " array([17], dtype=int32),\n",
       " array([29, 48, 24,  0, 29, 48, 33, 27,  2], dtype=int32),\n",
       " array([16,  4, 11,  7, 29,  4, 34,  5], dtype=int32),\n",
       " array([39, 48, 37, 14, 32,  7,  5,  1, 44, 27, 43, 18, 17], dtype=int32),\n",
       " array([31,  3, 35, 16,  1, 26, 16, 14], dtype=int32),\n",
       " array([11, 11, 42,  8, 14, 47, 33, 46,  9], dtype=int32),\n",
       " array([40,  5, 18,  0,  8, 16, 32, 44, 22,  1, 30, 42,  8, 19], dtype=int32),\n",
       " array([20,  7, 32, 43, 21, 23, 36, 16,  4, 17, 31, 20, 11, 19], dtype=int32),\n",
       " array([47, 10,  6,  2, 24, 33, 48, 21, 32, 22,  4,  0], dtype=int32),\n",
       " array([8, 5, 4], dtype=int32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(1, 50, 100, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hy, cy, ys = encoder(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "att = Attention(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(n_layers=1, n_target_vocab=100, n_units=100, attention=att, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20, 200)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(hy, cy, ys, xs)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda = Encoder_Decoder_withAttention(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable(34.225608825683594)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eda(xs,xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
