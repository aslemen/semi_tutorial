{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder-Decoderモデルの発展系として、Attentionを追加してみます。  \n",
    "ついでにEncocderのLSTMもBidirectional LSTMに変えてみましょう。\n",
    "\n",
    "残念ながら、2017年10月8日時点で、Attentionを一行で追加してくれるような機能はChainerにはありません。  \n",
    "Attentionの構造とChainerの関数などをきちんと理解し、自分で実装していくことになります。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ライブラリのインポート、idの系列をembeddingの系列に変える関数は先と一緒です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可変長単語id系列を可変長単語ベクトル系列へ写像する関数\n",
    "def sequence_embed(embed, xs):\n",
    "    x_len = [len(x) for x in xs]\n",
    "    x_section = np.cumsum(x_len[:-1])\n",
    "    ex = embed(F.concat(xs, axis=0))\n",
    "    exs = F.split_axis(ex, x_section, 0)\n",
    "    return exs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "せっかくAttentionを使うので、EncoderのLSTMをBidirectionalにしてみます。  \n",
    "Bidirectional LSTMは、すでにChainerに用意されているので、そこを変えるだけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Chain):\n",
    "    def __init__(self, n_layers, n_source_vocab, n_units, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.source_embed = L.EmbedID(in_size=n_source_vocab, out_size=n_units)\n",
    "            \n",
    "            # NStepLSTMをNStepBiLSTMに\n",
    "            self.encoder_lstm = L.NStepBiLSTM(n_layers=n_layers, in_size=n_units,\n",
    "                                              out_size=n_units, dropout=dropout)\n",
    "\n",
    "            self.n_source_vocab = n_source_vocab\n",
    "            self.n_units = n_units\n",
    "    \n",
    "    def __call__(self, source_xs):\n",
    "        # 単語の系列を単語ベクトルへ\n",
    "        exs = sequence_embed(self.source_embed, source_xs)\n",
    "        \n",
    "        # lstmの初期状態\n",
    "        hx = None\n",
    "        cx = None\n",
    "        \n",
    "        # lstmで各系列をエンコード\n",
    "        hy, cy, ys = self.encoder_lstm(hx, cx, exs)\n",
    "        return hy, cy, ys\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意としては、出力のhy, cyのshapeが変わります。  \n",
    "n_layerのaxisが、forward LSTMとbackward LSTMの分を合わせ、n_layer * 2になります。  \n",
    "また、ysの各隠れ層の次元数も2倍になります。  \n",
    "よってDecoderのLSTMの隠れ層の次元数はEmbeddingやEncoderのn_unitsの二倍になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attentionを導入します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はAttentionが機械翻訳に初めて適用された、 \n",
    "\n",
    "・[Bahdanau et al. (2015) Neural Machine Translation by Jointly Learning Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)  \n",
    "\n",
    "のモデルを書いていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![attention.png](https://raw.githubusercontent.com/kwashio/semi_tutorial/images/attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像は[スタンフォード大学の授業のスライド](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture10.pdf)から拝借しました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bahdanauのモデルでは、AttentionはLSTMの一番深い層（画像では一番上の層）で展開され、Decoderの隠れ状態を計算する際に使用されます。  \n",
    "具体的には、$h_t$を計算する際に、\n",
    "\n",
    "1. 一つ前の隠れ状態$h_{t-1}$からContext vector $c_t$を計算\n",
    "1. $h_{t-1}$、$c_t$、input vector（画像だと$h_t$の下のベクトル）から$h_t$を計算\n",
    "\n",
    "という風になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のクラスAttentionでは、$h_{t-1}$から$a_t$を計算し、$c_t$を出力するまでの処理を記述しています。 \n",
    "\n",
    "式にすると、Encoderのある隠れ状態の$score_i$は、\n",
    "\n",
    "\\begin{equation}\n",
    "score_i = softmax(W_2 z_i)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "z_i = tanh(W_1 (e_i \\oplus h_{t-1}))\n",
    "\\end{equation}\n",
    "\n",
    "ただし、$e_i$はEncoderの時点$i$における隠れ状態です。$\\oplus$はベクトルの結合を表しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装は以下の記事などを参考にしました。  \n",
    "[今更ながらchainerでSeq2Seq（2）〜Attention Model編〜](https://qiita.com/kenchin110100/items/eb70d69d1d65fb451b67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Chain):\n",
    "    def __init__(self, n_units):\n",
    "        super(Attention, self).__init__()\n",
    "        with self.init_scope():\n",
    "            #eWとdWで上の式のW1を表している。\n",
    "            # Encoder(BiLSTM)の隠れ状態の線形変換\n",
    "            self.eW = L.Linear(n_units*2) # Decoderの隠れ層の次元数はEncoderのn_unitsの二倍\n",
    "            \n",
    "            # 一つ前のdecoder中間層の線形変換\n",
    "            self.dW = L.Linear(n_units*2)\n",
    "            \n",
    "            \n",
    "            # スコア計算用の線形変換、上の式のW2\n",
    "            self.aW = L.Linear(1)\n",
    "    \n",
    "    def __call__(self, ehs, dh):\n",
    "        # 各z_iの計算\n",
    "        encoder_hidden = self.eW(ehs)\n",
    "        \n",
    "        # h_{t-1}の線形変換後のベクトルをbroadcastし、コピーしてencoder_hiddenに足し合わせる。\n",
    "        decoder_hidden = F.broadcast_to(self.dW(dh), encoder_hidden.shape)\n",
    "        attention_hidden = F.tanh(encoder_hidden + decoder_hidden)\n",
    "        \n",
    "        # scoreの計算。\n",
    "        scores = F.softmax(self.aW(attention_hidden), axis=0)\n",
    "        \n",
    "        # context vectorの計算\n",
    "        context = F.matmul(F.transpose(scores), ehs)\n",
    "        # (1 , n_units*2)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder(with Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attentionを考慮したDecoderを書いていきます。  \n",
    "少し複雑ですが、頑張ってついてきてください。  \n",
    "\n",
    "Attentionなしの単純なEncoder-Decoderモデルを書いたときは、EncoderもDecoderもNStepLSTMで書くことができました。  \n",
    "しかし、Attentionを考慮する場合は、Decoderの一番深いレイヤーの各隠れ状態を計算する際に、context vector $c_t$を計算に入れる必要があるため、単純にNStepLSTMを用いることはできません。\n",
    "\n",
    "つまり、一番深いレイヤーの計算部分は自分で書かなければなりません。  \n",
    "もう一度、さきほどの図を見てみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![attention2.png](https://github.com/kwashio/semi_tutorial/blob/images/attention2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$c_t$を除く青い隠れ状態の部分はEncoderで計算済みです。  \n",
    "赤色の隠れ状態はDecoderで計算するのですが、多層のLSTMを考えた時、赤枠の部分はNStepLSTMで計算できます。  \n",
    "つまり、自分で書かなければいけないのは、Decoderのトップの層の部分ということになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoderもトップの層もLSTMなのですが、通常のLSTMとは異なり、$h_{t-1}$とinput以外にcontext vector $c_t$を考慮したLSTMです。  \n",
    "このようなLSTMを記述する際は、レイヤーのLSTMやNStepLSTMではなく、[chainer.functions.lstm](https://docs.chainer.org/en/stable/reference/generated/chainer.functions.lstm.html#chainer.functions.lstm)を使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functionのLSTMは、大雑把に入力と出力を書くと、\n",
    "```\n",
    "c, h = lstm(previous_c, W1*previous_h + W2*input)\n",
    "```\n",
    "という風になっています。つまり、一つ前のcellと隠れ状態、input vectorを渡すと、新しいcellと隠れ状態を返してくれる関数です。  \n",
    "注意点としては、previous_h、 input vectorはlstm関数に入力される前に、Linearレイヤー（W1, W2）により中間層の4倍の次元のベクトルに変換されなければないことです。  \n",
    "なぜ4倍なのかというと、これはLSTMの各構成要素である、input gate、forget gate、output gate、new memory cellの計算に対応しています。\n",
    "\n",
    "このlstm関数はレイヤーのLSTMと異なり、内部にパラメータを持っておらず、計算処理のみを担っています。  \n",
    "LSTMとしてのパラメータは上の式における、W1とW2に相当します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ざっくりとlstm関数を理解したところで、ここにcontext vector $c_t$を組み込みます。  \n",
    "これは、以下のように行います。\n",
    "\n",
    "```\n",
    "c, h = lstm(previous_c, W1*previous_h + W2*input + W3*context)\n",
    "```\n",
    "\n",
    "これにより、context vectorを考慮しつつ、新たなcellと隠れ状態を計算することができます。  \n",
    "では、Decoderクラスを書いていきましょう。\n",
    "後々の処理過程を前に書いたAttentionなしのEncoder-Decoderモデルに合わせるため、出力はNStepLSTMと同じになるようにします。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Chain):\n",
    "    \n",
    "    def __init__(self, n_layers, n_target_vocab, n_units, attention, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.n_target_vocab = n_target_vocab\n",
    "            self.target_embed = L.EmbedID(n_target_vocab, n_units, dropout)\n",
    "            self.n_layers = n_layers\n",
    "            \n",
    "            # layer数が１かそれ以上かで、NStepLSTMを使うかどうか分岐\n",
    "            if self.n_layers > 1:\n",
    "                self.pre_lstm = L.NStepLSTM(self.n_layers -1, n_units, n_units*2, dropout)\n",
    "            \n",
    "            # attention\n",
    "            self.Att = attention\n",
    "            \n",
    "            self.dropout = dropout\n",
    "            \n",
    "            # topのLSTMの各線形変換（W1, W2, W3）\n",
    "            # 中間層がn_units*2なので、それを４倍にする。\n",
    "            self.lstm_input = L.Linear(n_units * 8)\n",
    "            self.lstm_previous = L.Linear(n_units * 8)\n",
    "            self.lstm_context = L.Linear(n_units * 8)\n",
    "            \n",
    "        \n",
    "    def __call__(self, hy, cy, ys, target_xs):\n",
    "        # hy, cy, ysはEncoder(BiLSTM)の出力\n",
    "        # target_xsは、単語idの系列のリスト\n",
    "        \n",
    "        # attention以外の部分の計算\n",
    "        batch_size = len(ys)\n",
    "        \n",
    "        # ターゲット言語の単語idの系列のリストを、embeddingの系列のリストへ\n",
    "        exs = sequence_embed(self.target_embed, target_xs)\n",
    "        \n",
    "        # EncoderのBiLSTMのhy, cyのshapeをDecoderの構造に合わせる。\n",
    "        # (n_layers*2, batchsize, n_units) -> (n_layers, batchsize, n_units*2)へ\n",
    "        hy = F.reshape(hy, (self.n_layers, batch_size, -1))\n",
    "        cy = F.reshape(cy, (self.n_layers, batch_size, -1))\n",
    "        \n",
    "        # n_layersが２以上のときは、NStepLSTMにより、一番深い層以外の隠れ状態を計算しておく。\n",
    "        if self.n_layers > 1:\n",
    "            unatt_n_layer = self.n_layers - 1\n",
    "            pre_hy = hy[:unatt_n_layer]\n",
    "            pre_cy = cy[:unatt_n_layer]\n",
    "            after_h, after_c, pre_os = self.pre_lstm(pre_hy, pre_cy, exs)\n",
    "        \n",
    "        else:\n",
    "            pre_os = exs\n",
    "        # pre_osが、一番深い層のLSTMへのinputの系列になる。\n",
    "        \n",
    "        # 最終層の計算\n",
    "        high_hy = hy[self.n_layers - 1]\n",
    "        high_cy = cy[self.n_layers - 1]\n",
    "        \n",
    "        # NStepLSTMと出力を合わせるために、リストを３つ用意\n",
    "        last_h = [] # 最後の隠れ状態のリスト\n",
    "        last_c = [] # 最後のcellのリスト\n",
    "        os = [] # 隠れ状態の系列のリスト\n",
    "        \n",
    "        # 各input系列ごとに処理\n",
    "        for i, pre_eos in enumerate(pre_os):\n",
    "            h = F.reshape(high_hy[i], (1,-1))\n",
    "            c = F.reshape(high_hy[i], (1,-1))\n",
    "            now_ys = ys[i] # 対応するEncoderの隠れ状態の系列 (lenght, n_units*2)\n",
    "            temp_os = []\n",
    "            \n",
    "            # verticalにdropoutがかかるので、dropoutをかける場所はここ\n",
    "            pre_eos = F.dropout(pre_eos, self.dropout)\n",
    "            \n",
    "            \n",
    "            for x in pre_eos:\n",
    "                # input vector\n",
    "                x = F.reshape(x, (1,-1))\n",
    "                \n",
    "                # 一つ前の隠れ状態からcontext vectorを計算。\n",
    "                context = self.Att(now_ys, h)\n",
    "                \n",
    "                # 次のセルと隠れ状態を計算する。\n",
    "                c, h = F.lstm(c,\n",
    "                              self.lstm_input(x) + self.lstm_previous(h) + self.lstm_context(context))\n",
    "                \n",
    "                # 隠れ状態を保存\n",
    "                temp_os.append(h)\n",
    "            \n",
    "            # 最後の隠れ状態、最後のセル、隠れ状態の系列を保存\n",
    "            last_h.append(h)\n",
    "            last_c.append(c)\n",
    "            os.append(F.concat(temp_os, axis=0))\n",
    "        \n",
    "        # 出力をNStepLSTMに合わせるために、shapeを変換\n",
    "        last_h = F.reshape(F.concat(last_h, axis=0), (1, batch_size, -1))\n",
    "        last_c = F.reshape(F.concat(last_c, axis=0), (1, batch_size, -1))\n",
    "        \n",
    "        # n_layerが２以上のときは、NStepLSTMの出力とconcatする。\n",
    "        if self.n_layers > 1:\n",
    "            ho = F.concat([after_h, last_h], axis=0)\n",
    "            co = F.concat([after_c, last_c], axis=0)\n",
    "        else:\n",
    "            ho = last_h\n",
    "            co = last_c\n",
    "            \n",
    "        \n",
    "        return ho, co, os\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "お疲れ様でした。  \n",
    "頑張って出力の形を揃えたので、あとの処理はAttentionなしのEncoder-Decoderモデルとほぼ同じです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder+Attentionモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = 0\n",
    "EOS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Decoder_withAttention(Chain):\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Encoder_Decoder_withAttention, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "        \n",
    "            self.W = L.Linear(self.decoder.n_target_vocab)\n",
    "            \n",
    "    def __call__(self, xs, target_xs):\n",
    "        \n",
    "        eos = self.xp.array([EOS], 'i')\n",
    "        target_in = [F.concat([eos, y], axis=0) for y in target_xs]\n",
    "        target_out = [F.concat([y, eos], axis=0) for y in target_xs]\n",
    "        \n",
    "        hy, cy, ys = self.encoder(xs)\n",
    "        _, _, os = self.decoder(hy, cy, ys, target_in)\n",
    "        \n",
    "        # loss calculation\n",
    "        batch_size = len(xs)\n",
    "        concat_os = F.concat(os, axis=0)\n",
    "        concat_target_out = F.concat(target_out, axis=0)\n",
    "        loss = F.sum(F.softmax_cross_entropy(\n",
    "            self.W(concat_os), concat_target_out, reduce='no')) / batch_size\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def translate(self, xs, max_length = 100):\n",
    "        batch_size = len(xs)\n",
    "        \n",
    "        with chainer.no_backprop_mode(), chainer.using_config('train', False):\n",
    "            # Encode\n",
    "            hy, cy, ys = self.encoder(xs)\n",
    "            \n",
    "            # decode時のinput用にbatch_size分のEOS=1を用意\n",
    "            target_xs = self.xp.full((batch_size,1), 1, 'i')\n",
    "            result = []\n",
    "            \n",
    "            ho = hy\n",
    "            co = cy\n",
    "            for i in range(max_length):\n",
    "                # 翻訳（予測）の際は、Encoderの出力をdecoderに入れる(ys)\n",
    "                ho, co, os = self.decoder(ho, co, ys, target_xs) \n",
    "                \n",
    "                concat_os = F.concat(os, axis=0)\n",
    "                wy = self.W(concat_os)\n",
    "                target_xs = self.xp.argmax(wy.data, axis=1).astype('i')\n",
    "                result.append(target_xs)\n",
    "                target_xs = F.reshape(target_xs, (-1, 1)).data\n",
    "            \n",
    "            result = self.xp.stack(result).T\n",
    "            # Remove EOS tags\n",
    "            outs = []\n",
    "            for y in result:\n",
    "                inds = np.argwhere(y == EOS)\n",
    "                if len(inds) > 0:\n",
    "                    y = y[:inds[0, 0]]\n",
    "                outs.append(y)\n",
    "        return outs\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実際に動かす"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでも実際に動かしてみます。  \n",
    "Attentionなしのモデルを動かしたときは１００文対のみで訓練しましたが、今回は前よりたくさんデータを使って実験してみましょう。  \n",
    "１５００文対で学習してみます。  \n",
    "先と同じく、日本語はMecabで分かち書き、英語は小文字化しておきます。\n",
    "\n",
    "今回も英日翻訳をやっていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は訓練データから、適当に３つ文を抜き出し、validation dataとします。  \n",
    "validation dataで数epochごとに翻訳の品質を試してみます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ読み込み、前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データ\n",
    "with open('data_full/ja.txt','r') as f:\n",
    "    ja = f.read().strip().split('\\n')\n",
    "\n",
    "with open('data_full/en.txt','r') as f:\n",
    "    en = f.read().strip().split('\\n')\n",
    "    \n",
    "ja = [s.strip() for s in ja][:1500]\n",
    "en = [s.strip() for s in en][:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validationデータ\n",
    "with open('data_full/val_ja.txt','r') as f:\n",
    "    val_ja = f.read().strip().split('\\n')\n",
    "\n",
    "with open('data_full/val_en.txt','r') as f:\n",
    "    val_en = f.read().strip().split('\\n')\n",
    "    \n",
    "val_ja = [s.strip() for s in val_ja]\n",
    "val_en = [s.strip() for s in val_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データで単語id辞書作成\n",
    "ja_w2id = {'UNK':0, 'EOS':1}\n",
    "en_w2id = {'UNK':0, 'EOS':1}\n",
    "\n",
    "ja_vocab = set()\n",
    "for s in ja:\n",
    "    ja_vocab.update(s.split(' '))\n",
    "    \n",
    "en_vocab = set()\n",
    "for s in en:\n",
    "    en_vocab.update(s.split(' '))\n",
    "\n",
    "for i, w in enumerate(ja_vocab):\n",
    "    ja_w2id[w] = i+2\n",
    "    \n",
    "for i, w in enumerate(en_vocab):\n",
    "    en_w2id[w] = i+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語id辞書保存\n",
    "import pickle\n",
    "with open('data_full/ja_w2id.dump', 'wb') as f:\n",
    "    pickle.dump(ja_w2id, f)\n",
    "\n",
    "with open('data_full/en_w2id.dump', 'wb') as f:\n",
    "    pickle.dump(en_w2id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語系列をid系列に変換\n",
    "ja_data = []\n",
    "for s in ja:\n",
    "    s = s.split(' ')\n",
    "    s = [ja_w2id[w] for w in s]\n",
    "    ja_data.append(np.array(s, 'i'))\n",
    "\n",
    "en_data = []\n",
    "for s in en:\n",
    "    s = s.split(' ')\n",
    "    s = [en_w2id[w] for w in s]\n",
    "    en_data.append(np.array(s, 'i'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation dataも\n",
    "val_ja_data = []\n",
    "for s in val_ja:\n",
    "    s = s.split(' ')\n",
    "    s = [ja_w2id.get(w, 0) for w in s]\n",
    "    val_ja_data.append(np.array(s, 'i'))\n",
    "    \n",
    "val_en_data = []\n",
    "for s in val_en:\n",
    "    s = s.split(' ')\n",
    "    s = [en_w2id.get(w, 0) for w in s]\n",
    "    val_en_data.append(np.array(s, 'i'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2601,  275,    0], dtype=int32),\n",
       " array([ 885,  377, 2779,    0,    0], dtype=int32),\n",
       " array([1846, 2438, 1888,    0, 1279, 1888, 1043,    0], dtype=int32)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_en_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は英語のvalidation dataに未知語（UNK＝０）があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i give advice.', '私 が アドバイス を する')\n",
      "('he listened to today’s news.', '彼 は 今日 の ニュース を 聞い た')\n",
      "('we use the trees in the japanese mountains.', 'われわれ は 日本 の 山 の 木 を 使い ます')\n"
     ]
    }
   ],
   "source": [
    "for s in zip(val_en, val_ja):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル用意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は以下のようなモデルを学習してみましょう。\n",
    "\n",
    "- レイヤー数は2.\n",
    "- n_units(embeddingの次元数)は50.\n",
    "- dropout rateは0.2\n",
    "- 最適化はAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 2\n",
    "n_units = 50\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 英語の文のEncoder\n",
    "encoder = Encoder(n_layers=n_layers, n_source_vocab=len(en_w2id),\n",
    "                  n_units=n_units, dropout=dropout)\n",
    "\n",
    "# Attention\n",
    "attention = Attention(n_units=n_units)\n",
    "\n",
    "# 日本語の文のDecoder\n",
    "decoder = Decoder(n_layers=n_layers, n_target_vocab=len(ja_w2id),\n",
    "                  n_units=n_units, attention=attention, dropout=dropout)\n",
    "\n",
    "# Encoder-Decoderモデル。\n",
    "model = Encoder_Decoder_withAttention(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(0.01)\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、20epochほど回してみて、5epochごとにどれくらいうまくフィッティングできているかを確かめましょう。  \n",
    "今回はミニバッチサイズは１００としましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idから単語への辞書。\n",
    "ja_id2w = {i:w for w, i in ja_w2id.items()}\n",
    "en_id2w = {i:w for w, i in en_w2id.items()}\n",
    "\n",
    "# 中間報告用の関数\n",
    "def translate_print(en_array, ja_array, en_id2w, ja_id2w):\n",
    "    print('=====中間報告=====')\n",
    "    for en, ja in zip(en_array, ja_array):\n",
    "        en_s = [en_id2w[i] for i in en]\n",
    "        print(' '.join(en_s))\n",
    "        \n",
    "        ja_s = [ja_id2w[i] for i in ja]\n",
    "        print('>>> '+' '.join(ja_s))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# epoch: 1, loss: 947.2382125854492\n",
      "# epoch: 2, loss: 813.2485122680664\n",
      "# epoch: 3, loss: 782.7784729003906\n",
      "# epoch: 4, loss: 763.1520195007324\n",
      "# epoch: 5, loss: 746.2089996337891\n",
      "=====中間報告=====\n",
      "i give UNK\n",
      ">>> 彼 が の の\n",
      "he listened to UNK UNK\n",
      ">>> 彼 が の の を\n",
      "we use the UNK in the japanese UNK\n",
      ">>> 彼 が の の の を\n",
      "\n",
      "\n",
      "# epoch: 6, loss: 729.8228607177734\n",
      "# epoch: 7, loss: 712.6900482177734\n",
      "# epoch: 8, loss: 692.3330307006836\n",
      "# epoch: 9, loss: 655.3534851074219\n",
      "# epoch: 10, loss: 611.0483818054199\n",
      "=====中間報告=====\n",
      "i give UNK\n",
      ">>> 私 が 人 に い ます\n",
      "he listened to UNK UNK\n",
      ">>> 彼 が 、 、 人 を ます\n",
      "we use the UNK in the japanese UNK\n",
      ">>> 私 は 、 、 人 に い ます\n",
      "\n",
      "\n",
      "# epoch: 11, loss: 580.491626739502\n",
      "# epoch: 12, loss: 554.8106422424316\n",
      "# epoch: 13, loss: 534.7021408081055\n",
      "# epoch: 14, loss: 513.9462642669678\n",
      "# epoch: 15, loss: 492.64785957336426\n",
      "=====中間報告=====\n",
      "i give UNK\n",
      ">>> 私 が 私 に い ます\n",
      "he listened to UNK UNK\n",
      ">>> 彼 が X を し ます\n",
      "we use the UNK in the japanese UNK\n",
      ">>> 私 は 、 、 １ ０ ０ ０ 人 に は い ます\n",
      "\n",
      "\n",
      "# epoch: 16, loss: 473.0758934020996\n",
      "# epoch: 17, loss: 454.49193954467773\n",
      "# epoch: 18, loss: 437.23913764953613\n",
      "# epoch: 19, loss: 420.17822456359863\n",
      "# epoch: 20, loss: 402.90794372558594\n",
      "=====中間報告=====\n",
      "i give UNK\n",
      ">>> 私 が 私 に は い ます\n",
      "he listened to UNK UNK\n",
      ">>> 彼 が X を 持っ て い ます\n",
      "we use the UNK in the japanese UNK\n",
      ">>> 当社 は 、 １ ０ ０ ０ ０ し ます\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 訓練データ総数\n",
    "n_train = len(ja_data)\n",
    "\n",
    "# ミニバッチサイズ\n",
    "batchsize = 100\n",
    "\n",
    "# エポック数\n",
    "n_epoch =20\n",
    "\n",
    "# ミニバッチ化のために、array化しておく。dtypeはobject\n",
    "ja_data = np.array(ja_data)\n",
    "en_data = np.array(en_data)\n",
    "\n",
    "for epoch in range(n_epoch): # epochのループ\n",
    "    sum_loss = 0\n",
    "    # ミニバッチの用意。\n",
    "    perm = np.random.permutation(n_train)\n",
    "    for i in range(0, n_train, batchsize):\n",
    "        ja_batch = ja_data[perm[i:i+batchsize]]\n",
    "        en_batch = en_data[perm[i:i+batchsize]]\n",
    "        \n",
    "        # lossの計算\n",
    "        loss = model(en_batch, ja_batch)\n",
    "        sum_loss += loss.data\n",
    "        # backpropatgation\n",
    "        optimizer.target.cleargrads() # 勾配のリセット\n",
    "        loss.backward() # 逆伝搬する誤差を計算\n",
    "        optimizer.update() # パラメータを更新\n",
    "    \n",
    "    log = '# epoch: {}, loss: {}'.format(epoch+1, sum_loss)\n",
    "    print(log)\n",
    "    \n",
    "    # 5epochごとに翻訳を出力。\n",
    "    if (epoch+1)%5 == 0:\n",
    "        result = model.translate(val_en_data)\n",
    "        translate_print(val_en_data, result, en_id2w, ja_id2w)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はここで打ち切ります。  \n",
    "lossは下がっていってますが、フィッティングはまだまだという感じです。  \n",
    "ボキャブラリが増えたからかもしれません。  \n",
    "一応、最初は非文が生成されていますが、だんだん日本語の文らしきものが生成されるようにはなっていってます。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "お疲れ様でした。  \n",
    "Attentionを実装してみました。  \n",
    "\n",
    "今回のように、フレームワークに予め用意されていないコンポーネントを実装するのは大変ですが、研究では確実に必要になってきます。  \n",
    "そのようなときは次のプロセスを踏むと良いでしょう。\n",
    "\n",
    "1. 実装したいモデルの構造を、論文を読み込んだりしてちゃんと理解する。\n",
    "2. 誰かが実装している場合はそれをパクる。\n",
    "3. 無い場合は自分で書く。\n",
    "\n",
    "２において、自分がメインで使っているフレームワークとは別のフレームワークで実装されている、ということもあると思います。  \n",
    "そのようなときに、色んなフレームワークについて最低限コードは読めるぐらいになっておくと良いでしょう。  \n",
    "自分で書くのはともかく、読めるようになるコストは低いと思います。\n",
    "\n",
    "最近ではChainerと似た思想で作られたフレームワークであるPyTorchによる実装などが増えているようです。  \n",
    "書き方はChainerとほぼ同じですし、チュートリアルもとても充実しているので、まずこのあたりに触れてみるのはいかがでしょうか。  \n",
    "http://pytorch.org/docs/master/  \n",
    "http://pytorch.org/tutorials/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
